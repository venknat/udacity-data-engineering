# Udacity Data Engineering Nanodegree Data Warehouse Project

## Purpose

The purpose of this pipeline is to take Sparkify log data of its users song plays, together with data
about the songs it has available, and allow us to analyze the data for patterns that might occur in the
play list (which songs are the most popular?  What artists?  Are there trends that are based on time,
such as whether certain songs or artists are more popular at certain days of the week or certain times of the year).

The data possessed by sparkify right now is just in json blobs, and so is hard to analyze for these questions.
This pipeline will convert the json blobs stored in S3 into a star-schema database on redshift where we
can perform such queries.


## Database Schema

The key events that our analysis covers are song plays by our users.  Designing our database using our star
schema, then, the song plays themselves will be the fact table in our database, called `songplays`.
Useful metadata about these  plays include the user that played the song, the song that was played, the artist of that
song, and the time  it was played.  Thus, we will create four dimension tables: `songs`, `artists`, `times`,
and `users`.

Since there are relatively few distinct songs, artists, and users, these three tables will be distributed
to all notes of our redshift cluster.

However, there will be many songplays and each of those will give rise to what's a (mostly, but not necessarily)
unique point in time.  It is also likely we will want to access the song play in conjunction with its
associated  timestamp (in order to do analysis regarding _when_ the song was played).  Therefore, we will 
distribute both  the `songplays` table and the `times` table by the `start_time` column (each table has this column in it).

The table definitions are as follows:

```
songplay_table_create = ("""
    CREATE TABLE IF NOT EXISTS songplays (
        songplay_id integer GENERATED BY DEFAULT AS IDENTITY (0,1),
        start_time bigint,
        user_id integer,
        level varchar(10),
        song_id varchar(25),
        artist_id varchar(25),
        session_id integer,
        location varchar(255),
        user_agent varchar(255),
        primary key(songplay_id),
        foreign key(song_id) references songs(song_id),
        foreign key(artist_id) references artists(artist_id),
        foreign key(user_id) references users(user_id),
        foreign key(start_time) references times(start_time)
    ) DISTKEY(start_time);
""")

user_table_create = ("""
    CREATE TABLE IF NOT EXISTS users (
        user_id integer,
        first_name varchar(255) NOT NULL,
        last_name varchar(255) NOT NULL,
        gender varchar(10),
        level varchar(10),
        primary key(user_id)
    ) DISTSTYLE ALL;
""")

song_table_create = ("""
    CREATE TABLE IF NOT EXISTS songs (
        song_id varchar(25),
        title varchar(1023) NOT NULL,
        artist_id varchar(25),
        year integer,
        duration decimal(7,2),
        primary key(song_id),
        foreign key(artist_id) references artists(artist_id)
    ) DISTSTYLE ALL;
""")

artist_table_create = ("""
    CREATE TABLE IF NOT EXISTS artists (
        artist_id varchar(25),
        name varchar(255) NOT NULL,
        location varchar(50),
        latitude double precision,
        longitude double precision,
        primary key(artist_id)
    ) DISTSTYLE ALL;
""")

time_table_create = ("""
    CREATE TABLE IF NOT EXISTS times (
        start_time bigint,
        hour integer NOT NULL,
        day integer NOT NULL,
        week integer NOT NULL,
        month integer NOT NULL,
        year integer NOT NULL,
        weekday integer NOT NULL,
        primary key(start_time)
    ) DISTKEY(start_time);
""")
```

## Instructions

### Required Packages

* psycopg2
* boto3
* pandas

### Set Up IAM Users

To run this, you will need to be able to act as IAM user(s) with access to the following permissions
within a root account that you own.  This script will not attempt to create such users.

* `AdministratorAccess` to create roles within your root account.  This is only needed if you intend
  to run the `create_redshift_db.py` script.  If not, you can omit this.
* `AmazonS3ReadOnlyAccess` to read the data from S3

A template iam.cfg file has been provided, and looks like this.

```
[ROOT_USER]
ACCESS_ID_KEY=aws id key here
ACCESS_ID_SECRET=aws id secret here

[S3_READER]
ACCESS_ID_KEY=aws id key here
ACCESS_ID_SECRET=aws id secret here
```

Add the aws id key/sercrets to these lines.

### Set Up Redshift Cluster and Security Group

You'll need a redshift cluster, configured so that one can access it using a db group.

Provided is a script `create_redshift_db.py` that can do this programmatically for you.  To use this,
fill in the `dwh.config` file, replacing all square-bracketed values with your desired values.

The lines in this file that _don't_ correspond to square bracketed values in the following can be left alone,
or you can change them if you like (and know what you're doing).

```
[CLUSTER]
IDENTIFIER=[desired name of your redshift cluster here]
DB_NAME=[desired db name]
DB_USER=[desired db username]
DB_PASSWORD=[put secure password here]
DB_PORT=5439
CLUSTER_TYPE=multi-node
NUM_NODES=4
NODE_TYPE=dc2.large
HOST=

[IAM_ROLE]
ROLENAME=sparkifyReadS3Role
ARN=

[S3]
LOG_DATA='s3://udacity-dend/log-data'
LOG_JSONPATH='s3://udacity-dend/log_json_path.json'
SONG_DATA='s3://udacity-dend/song-data'
READER_ROLE_NAME='s3ReaderRole'
```

When `create_redshift_db.py` is done, a print out showing various properties of the cluster will appear.
Copy the role ARN name to the `ARN=` line of the `dwh.cfg` file and the endpoint shown in the
`DWH_ENDPOINT` line to the `HOST=` line of the dwh.cfg file.  You'll end up with something like:

```
[CLUSTER]
IDENTIFIER=redshift-cluster-name
DB_NAME=dbname
DB_USER=dbuser
DB_PASSWORD=password
DB_PORT=5439
CLUSTER_TYPE=multi-node
NUM_NODES=4
NODE_TYPE=dc2.large
HOST=redshift-cluster-name.cjpczyca8ch5.us-west-2.redshift.amazonaws.com

[IAM_ROLE]
ROLENAME=roleName
ARN=arn:aws:iam::234567890123:role/roleName

[S3]
LOG_DATA='s3://udacity-dend/log-data'
LOG_JSONPATH='s3://udacity-dend/log_json_path.json'
SONG_DATA='s3://udacity-dend/song-data'
READER_ROLE_NAME='s3ReaderRole'
```

NOTE: This might time out if you run it.  If so, try going to the security group of your redshift cluster
(accessible from your redshift dashboard) and allow an incoming rule of Type Redshift, Protocol TCP,
port 5439, source `0.0.0.0/0`, delete your redshift cluster, and retry the script.

The `create_redshift_db.py` script will link your redshift cluster to your default security group.

Alternatively, you can manually create your own redshift cluster using the AWS UI.  Make sure it is attached to a role
that has `amazonS3ReadOnlyAccess` to it, and is in a security group set up to allow all inbound redshift
connections, and fill in `dwh.cfg` with the proper information.


### Create DB Tables

Run `python create_tables.py`.  This will drop, and then recreate, the needed tables.  Note that this means
all existing data in your redshift cluster in these tables will be erased, so use with care.

### Load Data

To load the data, run `python etl.py.`  Under the full data set shown in the `S3` section of `dwh.cfg`,
this can take about two hours to run.


### Example Queries

To get which song ids are played most on a given day of the week:

```
SELECT s.id, COUNT(*) AS num_plays
FROM
songplays sp
JOIN times t ON sp.start_time = t.start_time
JOIN songs s ON sp.song_id = songs.song_id
WHERE t.weekday=2
GROUP BY s.id
ORDER BY num_plays DESC
LIMIT 10;
```

To get which artists have been played the most:

```
SELECT a.id, COUNT(*) as num_plays
FROM
songplays sp
JOIN artists a ON a.artist_id = sp.artist_id
GROUP BY a.id
ORDER BY num_plays DESC
LIMIT 10;
```
